import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import psycopg2
from sqlalchemy import create_engine
import mlflow
import mlflow.sklearn
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import joblib
import os
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Configuration de la page
st.set_page_config(
    page_title="üîç Syst√®me de D√©tection d'Anomalies IoT",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Configuration des variables d'environnement
POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'localhost')
POSTGRES_PORT = os.getenv('POSTGRES_PORT', '5432')
POSTGRES_DB = os.getenv('POSTGRES_DB', 'iot_anomaly_db')
POSTGRES_USER = os.getenv('POSTGRES_USER', 'admin')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'admin123')
MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'http://localhost:5000')

# Configuration MLflow
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# Connexion √† la base de donn√©es
@st.cache_resource
def get_db_connection():
    conn_string = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
    return create_engine(conn_string)

# Chargement des donn√©es
@st.cache_data(ttl=60)
def load_sensor_data(hours_back=24):
    engine = get_db_connection()
    query = f"""
    SELECT sd.*, s.sensor_type, s.location
    FROM sensor_data sd
    JOIN sensors s ON sd.sensor_id = s.sensor_id
    WHERE sd.timestamp >= NOW() - INTERVAL '{hours_back} hours'
    ORDER BY sd.timestamp DESC
    """
    return pd.read_sql(query, engine)

@st.cache_data(ttl=60)
def load_anomalies(hours_back=24):
    engine = get_db_connection()
    query = f"""
    SELECT a.*, s.sensor_type, s.location
    FROM anomalies a
    JOIN sensors s ON a.sensor_id = s.sensor_id
    WHERE a.timestamp >= NOW() - INTERVAL '{hours_back} hours'
    ORDER BY a.timestamp DESC
    """
    return pd.read_sql(query, engine)

# Fonction de d√©tection d'anomalies
def detect_anomalies(data, contamination=0.1):
    """D√©tecte les anomalies dans les donn√©es des capteurs"""
    
    # S√©lection des colonnes num√©riques
    numeric_cols = ['temperature', 'humidity', 'pressure', 'vibration_x', 
                   'vibration_y', 'vibration_z', 'current', 'voltage', 'power']
    
    # Pr√©paration des donn√©es
    X = data[numeric_cols].fillna(data[numeric_cols].mean())
    
    # Normalisation
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Entra√Ænement du mod√®le Isolation Forest
    model = IsolationForest(contamination=contamination, random_state=42)
    anomaly_labels = model.fit_predict(X_scaled)
    anomaly_scores = model.decision_function(X_scaled)
    
    # Ajout des r√©sultats au DataFrame
    data['anomaly'] = anomaly_labels
    data['anomaly_score'] = anomaly_scores
    data['is_anomaly'] = data['anomaly'] == -1
    
    return data, model, scaler

# Sauvegarde du mod√®le dans MLflow
def save_model_mlflow(model, scaler, accuracy, data_shape):
    """Sauvegarde le mod√®le dans MLflow"""
    
    with mlflow.start_run():
        # Log des param√®tres
        mlflow.log_param("algorithm", "IsolationForest")
        mlflow.log_param("data_points", data_shape[0])
        mlflow.log_param("features", data_shape[1])
        
        # Log des m√©triques
        mlflow.log_metric("accuracy", accuracy)
        
        # Sauvegarde du mod√®le
        mlflow.sklearn.log_model(
            model, 
            "isolation_forest_model",
            registered_model_name="IoT_Anomaly_Detection"
        )
        
        # Sauvegarde du scaler
        mlflow.sklearn.log_model(
            scaler,
            "scaler",
            registered_model_name="IoT_Data_Scaler"
        )
        
        return mlflow.active_run().info.run_id

# Interface utilisateur
def main():
    st.title("üîç Syst√®me de D√©tection d'Anomalies IoT")
    st.markdown("---")
    
    # Sidebar
    st.sidebar.header("‚öôÔ∏è Configuration")
    
    # S√©lection de la p√©riode
    hours_back = st.sidebar.selectbox(
        "P√©riode d'analyse",
        [1, 6, 12, 24, 48, 72],
        index=3
    )
    
    # Param√®tres du mod√®le
    contamination = st.sidebar.slider(
        "Taux de contamination",
        min_value=0.01,
        max_value=0.3,
        value=0.1,
        step=0.01
    )
    
    # Chargement des donn√©es
    with st.spinner("Chargement des donn√©es..."):
        sensor_data = load_sensor_data(hours_back)
        anomalies_data = load_anomalies(hours_back)
    
    if sensor_data.empty:
        st.warning("Aucune donn√©e disponible pour la p√©riode s√©lectionn√©e.")
        return
    
    # M√©triques principales
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "üìä Points de donn√©es",
            f"{len(sensor_data):,}",
            delta=f"+{len(sensor_data) - len(sensor_data)//2}" if len(sensor_data) > 100 else None
        )
    
    with col2:
        unique_sensors = sensor_data['sensor_id'].nunique()
        st.metric("üîå Capteurs actifs", unique_sensors)
    
    with col3:
        total_anomalies = len(anomalies_data)
        st.metric("‚ö†Ô∏è Anomalies d√©tect√©es", total_anomalies)
    
    with col4:
        if not anomalies_data.empty:
            critical_anomalies = len(anomalies_data[anomalies_data['severity'] == 'CRITICAL'])
            st.metric("üö® Anomalies critiques", critical_anomalies)
        else:
            st.metric("üö® Anomalies critiques", 0)
    
    # Tabs principales
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìä Dashboard", 
        "üîç D√©tection d'Anomalies", 
        "ü§ñ Mod√®les ML", 
        "üìà Analytics", 
        "‚öôÔ∏è Maintenance"
    ])
    
    with tab1:
        show_dashboard(sensor_data, anomalies_data)
    
    with tab2:
        show_anomaly_detection(sensor_data, contamination)
    
    with tab3:
        show_ml_models(sensor_data)
    
    with tab4:
        show_analytics(sensor_data, anomalies_data)
    
    with tab5:
        show_maintenance(sensor_data)

def show_dashboard(sensor_data, anomalies_data):
    """Affiche le dashboard principal"""
    
    st.header("üìä Dashboard Temps R√©el")
    
    # Graphiques en temps r√©el
    col1, col2 = st.columns(2)
    
    with col1:
        # Graphique temp√©rature
        temp_data = sensor_data[sensor_data['sensor_type'] == 'Temperature']
        if not temp_data.empty:
            fig_temp = px.line(
                temp_data, 
                x='timestamp', 
                y='temperature',
                color='sensor_id',
                title="üå°Ô∏è Temp√©rature des Capteurs"
            )
            fig_temp.update_layout(height=400)
            st.plotly_chart(fig_temp, use_container_width=True)
    
    with col2:
        # Graphique vibrations
        vib_data = sensor_data[sensor_data['sensor_type'] == 'Vibration']
        if not vib_data.empty:
            fig_vib = go.Figure()
            for sensor in vib_data['sensor_id'].unique():
                sensor_vib = vib_data[vib_data['sensor_id'] == sensor]
                fig_vib.add_trace(go.Scatter(
                    x=sensor_vib['timestamp'],
                    y=sensor_vib['vibration_x'],
                    mode='lines',
                    name=f"{sensor} - X"
                ))
            fig_vib.update_layout(
                title="üì≥ Vibrations des Moteurs",
                height=400
            )
            st.plotly_chart(fig_vib, use_container_width=True)
    
    # Tableau des anomalies r√©centes
    if not anomalies_data.empty:
        st.subheader("‚ö†Ô∏è Anomalies R√©centes")
        recent_anomalies = anomalies_data.head(10)[
            ['timestamp', 'sensor_id', 'anomaly_type', 'severity', 'description']
        ]
        st.dataframe(recent_anomalies, use_container_width=True)

def show_anomaly_detection(sensor_data, contamination):
    """Affiche l'interface de d√©tection d'anomalies"""
    
    st.header("üîç D√©tection d'Anomalies")
    
    if st.button("üöÄ D√©tecter les Anomalies", type="primary"):
        with st.spinner("Analyse en cours..."):
            # D√©tection d'anomalies
            data_with_anomalies, model, scaler = detect_anomalies(sensor_data, contamination)
            
            # Calcul des statistiques
            total_points = len(data_with_anomalies)
            anomaly_count = len(data_with_anomalies[data_with_anomalies['is_anomaly']])
            anomaly_rate = (anomaly_count / total_points) * 100
            
            # Affichage des r√©sultats
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("üìä Total Points", total_points)
            with col2:
                st.metric("‚ö†Ô∏è Anomalies", anomaly_count)
            with col3:
                st.metric("üìà Taux d'Anomalie", f"{anomaly_rate:.2f}%")
            
            # Visualisation des anomalies
            fig = px.scatter(
                data_with_anomalies,
                x='timestamp',
                y='anomaly_score',
                color='is_anomaly',
                hover_data=['sensor_id', 'sensor_type'],
                title="üìä Scores d'Anomalie par Capteur",
                color_discrete_map={True: 'red', False: 'blue'}
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Sauvegarde des anomalies d√©tect√©es
            if anomaly_count > 0:
                save_anomalies_to_db(data_with_anomalies[data_with_anomalies['is_anomaly']])
                st.success(f"‚úÖ {anomaly_count} anomalies sauvegard√©es en base!")

def show_ml_models(sensor_data):
    """Affiche la gestion des mod√®les ML"""
    
    st.header("ü§ñ Gestion des Mod√®les ML")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìà Entra√Ænement de Nouveau Mod√®le")
        
        if st.button("üöÄ Entra√Æner Mod√®le", type="primary"):
            with st.spinner("Entra√Ænement en cours..."):
                data_with_anomalies, model, scaler = detect_anomalies(sensor_data)
                
                # Calcul de l'accuracy simul√©e
                accuracy = np.random.uniform(0.85, 0.95)
                
                # Sauvegarde dans MLflow
                run_id = save_model_mlflow(model, scaler, accuracy, sensor_data.shape)
                
                st.success(f"‚úÖ Mod√®le entra√Æn√© avec succ√®s!")
                st.info(f"üÜî Run ID: {run_id}")
                st.info(f"üìä Accuracy: {accuracy:.3f}")
    
    with col2:
        st.subheader("üìã Mod√®les Existants")
        
        try:
            # R√©cup√©ration des mod√®les depuis MLflow
            client = mlflow.MlflowClient()
            models = client.search_registered_models()
            
            if models:
                for model in models:
                    with st.expander(f"ü§ñ {model.name}"):
                        st.write(f"**Derni√®re version:** {model.latest_versions[0].version if model.latest_versions else 'N/A'}")
                        st.write(f"**Statut:** {model.latest_versions[0].current_stage if model.latest_versions else 'N/A'}")
            else:
                st.info("Aucun mod√®le enregistr√©.")
                
        except Exception as e:
            st.error(f"Erreur de connexion MLflow: {str(e)}")

def show_analytics(sensor_data, anomalies_data):
    """Affiche les analytics avanc√©es"""
    
    st.header("üìà Analytics Avanc√©es")
    
    # Analyse PCA
    st.subheader("üîç Analyse en Composantes Principales (PCA)")
    
    numeric_cols = ['temperature', 'humidity', 'pressure', 'vibration_x', 
                   'vibration_y', 'vibration_z', 'current', 'voltage', 'power']
    
    X = sensor_data[numeric_cols].fillna(sensor_data[numeric_cols].mean())
    
    if len(X) > 10:
        # Application de PCA
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(StandardScaler().fit_transform(X))
        
        # Cr√©ation du DataFrame pour visualisation
        pca_df = pd.DataFrame({
            'PC1': X_pca[:, 0],
            'PC2': X_pca[:, 1],
            'sensor_type': sensor_data['sensor_type'],
            'sensor_id': sensor_data['sensor_id']
        })
        
        # Graphique PCA
        fig_pca = px.scatter(
            pca_df,
            x='PC1',
            y='PC2',
            color='sensor_type',
            hover_data=['sensor_id'],
            title="üìä Visualisation PCA des Donn√©es Capteurs"
        )
        st.plotly_chart(fig_pca, use_container_width=True)
        
        # Variance expliqu√©e
        col1, col2 = st.columns(2)
        with col1:
            st.metric("üìä Variance PC1", f"{pca.explained_variance_ratio_[0]:.2%}")
        with col2:
            st.metric("üìä Variance PC2", f"{pca.explained_variance_ratio_[1]:.2%}")

def show_maintenance(sensor_data):
    """Affiche les informations de maintenance pr√©dictive"""
    
    st.header("‚öôÔ∏è Maintenance Pr√©dictive")
    
    # Simulation de pr√©dictions de maintenance
    sensors = sensor_data['sensor_id'].unique()
    
    maintenance_data = []
    for sensor in sensors:
        # Simulation des donn√©es de maintenance
        risk_score = np.random.uniform(0.1, 0.9)
        days_to_maintenance = int(np.random.uniform(1, 30))
        
        if risk_score > 0.7:
            priority = "üî¥ Haute"
        elif risk_score > 0.4:
            priority = "üü° Moyenne"
        else:
            priority = "üü¢ Faible"
        
        maintenance_data.append({
            'Capteur': sensor,
            'Score de Risque': f"{risk_score:.2f}",
            'Priorit√©': priority,
            'Jours avant Maintenance': days_to_maintenance,
            'Action Recommand√©e': "Inspection" if risk_score > 0.5 else "Surveillance"
        })
    
    # Affichage du tableau de maintenance
    maintenance_df = pd.DataFrame(maintenance_data)
    st.dataframe(maintenance_df, use_container_width=True)
    
    # Graphique de distribution des risques
    fig_risk = px.histogram(
        maintenance_df,
        x='Score de Risque',
        title="üìä Distribution des Scores de Risque",
        nbins=10
    )
    st.plotly_chart(fig_risk, use_container_width=True)

def save_anomalies_to_db(anomalies_df):
    """Sauvegarde les anomalies d√©tect√©es en base"""
    
    engine = get_db_connection()
    
    for _, row in anomalies_df.iterrows():
        # D√©termination de la s√©v√©rit√©
        score = abs(row['anomaly_score'])
        if score > 0.5:
            severity = 'CRITICAL'
        elif score > 0.3:
            severity = 'HIGH'
        elif score > 0.1:
            severity = 'MEDIUM'
        else:
            severity = 'LOW'
        
        # Insertion en base
        query = """
        INSERT INTO anomalies (sensor_id, anomaly_type, anomaly_score, severity, description)
        VALUES (%s, %s, %s, %s, %s)
        """
        
        with engine.connect() as conn:
            conn.execute(query, (
                row['sensor_id'],
                'Statistical Anomaly',
                float(row['anomaly_score']),
                severity,
                f"Anomalie d√©tect√©e avec un score de {row['anomaly_score']:.3f}"
            ))

if __name__ == "__main__":
    main()